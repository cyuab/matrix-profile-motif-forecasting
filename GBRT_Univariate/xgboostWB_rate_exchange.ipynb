{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef81fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time series:  0\n",
      "Traing length : 6048\n",
      "Traing length : 1512\n",
      "Time series:  1\n",
      "Traing length : 6048\n",
      "Traing length : 1512\n",
      "Time series:  2\n",
      "Traing length : 6048\n",
      "Traing length : 1512\n",
      "Time series:  3\n",
      "Traing length : 6048\n",
      "Traing length : 1512\n",
      "Time series:  4\n",
      "Traing length : 6048\n",
      "Traing length : 1512\n",
      "Time series:  5\n",
      "Traing length : 6048\n",
      "Traing length : 1512\n",
      "Time series:  6\n",
      "Traing length : 6048\n",
      "Traing length : 1512\n",
      "Time series:  7\n",
      "Traing length : 6048\n",
      "Traing length : 1512\n",
      "48008\n",
      "24\n",
      "496\n",
      "24\n",
      "Fitting Done!\n",
      "RMSE:  0.019542774\n",
      "WAPE:  0.015714908\n",
      "MAE:  0.011746564\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"XGBoostWB-Rate_Exchange\"\"\"\n",
    "import sys\n",
    "\n",
    "sys.version\n",
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# %matplotlib inline\n",
    "import shutil\n",
    "from random import shuffle\n",
    "import itertools\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import xgboost as xgb\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import preprocessing\n",
    "\n",
    "num_periods_output = 24  # to predict\n",
    "num_periods_input = 24  # input\n",
    "\n",
    "ALL_Test_Data = []\n",
    "ALL_Test_Prediction = []\n",
    "\n",
    "\"\"\"## preprocessing\"\"\"\n",
    "\n",
    "\n",
    "def New_preprocessing(TimeSeries):\n",
    "    # print(len(TimeSeries))\n",
    "    Data = []\n",
    "    start_date = datetime(1990, 1, 1, 00, 00, 00)  # define start date\n",
    "    for i in range(0, len(TimeSeries)):\n",
    "        record = []\n",
    "        record.append(TimeSeries[i])  # adding the xchangerate value\n",
    "        record.append(start_date.month)\n",
    "        record.append(start_date.day)\n",
    "        # record.append(start_date.hour)\n",
    "        # record.append(start_date.minute)\n",
    "        record.append(start_date.weekday())\n",
    "        record.append(start_date.timetuple().tm_yday)\n",
    "        record.append(start_date.isocalendar()[1])\n",
    "        # print(start_date.month,' ',start_date.day,' ',start_date.hour,' ',start_date.weekday(),' ',start_date.timetuple().tm_yday,' ',start_date.isocalendar()[1])\n",
    "        start_date = start_date + timedelta(days=1)\n",
    "        # print('year',start_date.year,'Month:',start_date.month,' day:',start_date.day,' hour:',start_date.hour)\n",
    "        Data.append(record)\n",
    "    ########## change list of lists to df ################\n",
    "    headers = [\"pems\", \"month\", \"day\", \"day_of_week\", \"day_of_year\", \"week_of_year\"]\n",
    "    Data_df = pd.DataFrame(Data, columns=headers)\n",
    "    # print(Data_df)\n",
    "    sub = Data_df.iloc[:, 1:]\n",
    "    New_sub = preprocessing.minmax_scale(sub, feature_range=(-0.5, 0.5))\n",
    "    # Normalize features to be from -0.5 to 0.5 as mentioned in the paper\n",
    "    Normalized_Data_df = pd.DataFrame(\n",
    "        np.column_stack([Data_df.iloc[:, 0], New_sub]), columns=headers\n",
    "    )\n",
    "    # print(Normalized_Data_df)\n",
    "    #################################################################################################\n",
    "    # cut training and testing\n",
    "    train_split = np.floor(len(Normalized_Data_df) * 0.8)  # 60 % training\n",
    "    # train_split=180\n",
    "    train_split = int(\n",
    "        train_split - (train_split % (num_periods_output + num_periods_input))\n",
    "    )\n",
    "    # print('-------------------',train_split)\n",
    "    Train = Normalized_Data_df.iloc[0:train_split, :]\n",
    "    Train = Train.values\n",
    "    Train = Train.astype(\"float32\")\n",
    "    print(\"Traing length :\", len(Train))\n",
    "    total = len(Normalized_Data_df)\n",
    "    test_split = np.floor(len(Normalized_Data_df) * 0.2)  # 20 % testing\n",
    "    # test_split=20\n",
    "    # print('-------------------test: ',test_split)\n",
    "    test_split = int(\n",
    "        test_split - (test_split % (num_periods_output + num_periods_input))\n",
    "    )\n",
    "    Test = Normalized_Data_df.iloc[(total - test_split - num_periods_input) :, :]\n",
    "    Test = Test.values\n",
    "    Test = Test.astype(\"float32\")\n",
    "    print(\"Traing length :\", len(Test))\n",
    "    Number_Of_Features = 6\n",
    "    ############################################ Windowing ##################################\n",
    "    end = len(Train)\n",
    "    start = 0\n",
    "    next = 0\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    count = 0\n",
    "    # print('lennnn',len(Train))\n",
    "    limit = max(num_periods_input, num_periods_output)\n",
    "    while next + (limit) < end:\n",
    "        next = start + num_periods_input\n",
    "        x_batches.append(Train[start:next, :])\n",
    "        y_batches.append(Train[next : next + num_periods_output, 0])\n",
    "        start = start + 1\n",
    "    y_batches = np.asarray(y_batches)\n",
    "    y_batches = y_batches.reshape(-1, num_periods_output, 1)\n",
    "    # print('Length of y batches :',len(y_batches),' ',num_periods_input,' ',num_periods_output)\n",
    "    # print(x_batches)\n",
    "    x_batches = np.asarray(x_batches)\n",
    "    x_batches = x_batches.reshape(-1, num_periods_input, Number_Of_Features)\n",
    "    # print('len x_batches ',len(x_batches))\n",
    "    ############################################ Windowing ##################################\n",
    "    end_test = len(Test)\n",
    "    start_test = 0\n",
    "    next_test = 0\n",
    "    x_testbatches = []\n",
    "    y_testbatches = []\n",
    "    while next_test + (limit) < end_test:\n",
    "        next_test = start_test + num_periods_input\n",
    "        x_testbatches.append(Test[start_test:next_test, :])\n",
    "        y_testbatches.append(Test[next_test : next_test + num_periods_output, 0])\n",
    "        start_test = start_test + num_periods_input\n",
    "    y_testbatches = np.asarray(y_testbatches)\n",
    "    y_testbatches = y_testbatches.reshape(-1, num_periods_output, 1)\n",
    "    x_testbatches = np.asarray(x_testbatches)\n",
    "    x_testbatches = x_testbatches.reshape(-1, num_periods_input, Number_Of_Features)\n",
    "    # print(' xTestbatches',len(x_testbatches),' yTestbatches',len(y_testbatches))\n",
    "    return x_batches, y_batches, x_testbatches, y_testbatches\n",
    "\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"../data/exchange_rate.txt\", sep=\",\", header=None\n",
    ")\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "data = data.T\n",
    "# print(data.shape)\n",
    "# print('Number of timeseries: ',len(data))\n",
    "x_batches_Full = []\n",
    "y_batches_Full = []\n",
    "X_Test_Full = []\n",
    "Y_Test_Full = []\n",
    "for i in range(0, len(data)):\n",
    "    print(\"Time series: \", i)\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    X_Test = []\n",
    "    Y_Test = []\n",
    "    TimeSeries = data.iloc[i, :]\n",
    "    # New_preprocessing(TimeSeries)\n",
    "    # TimeSeries=np.arange(1,7588,1)\n",
    "    # print(TimeSeries[7586:])\n",
    "    x_batches, y_batches, X_Test, Y_Test = New_preprocessing(TimeSeries)\n",
    "    for element1 in x_batches:\n",
    "        x_batches_Full.append(element1)\n",
    "\n",
    "    for element2 in y_batches:\n",
    "        y_batches_Full.append(element2)\n",
    "\n",
    "    for element5 in X_Test:\n",
    "        X_Test_Full.append(element5)\n",
    "\n",
    "    for element6 in Y_Test:\n",
    "        Y_Test_Full.append(element6)\n",
    "    # ---------------------shuffle windows  X and target Y together-------------------------------------\n",
    "# print(len(x_batches_Full),'     length of all file : ',len(y_batches_Full))\n",
    "combined = list(zip(x_batches_Full, y_batches_Full))\n",
    "random.shuffle(combined)\n",
    "shuffled_batch_features, shuffled_batch_y = zip(*combined)\n",
    "\n",
    "# xgboost part\n",
    "print(len(x_batches_Full))\n",
    "All_Training_Instances = []\n",
    "\n",
    "# =============== flatten each training window into Instance =================================\n",
    "for i in range(0, len(shuffled_batch_features)):\n",
    "    hold = []\n",
    "    temp = []\n",
    "    for j in range(0, len(shuffled_batch_features[i])):\n",
    "        # **************** to run without features --> comment if else condition **************************\n",
    "        # print(len(hold))\n",
    "        if j == (len(shuffled_batch_features[i]) - 1):\n",
    "            # hold = np.concatenate((hold, shuffled_batch_features[i][j][:]), axis=None)\n",
    "            hold = np.concatenate((hold, shuffled_batch_features[i][j][0]), axis=None)\n",
    "        else:\n",
    "            hold = np.concatenate((hold, shuffled_batch_features[i][j][0]), axis=None)\n",
    "\n",
    "    All_Training_Instances.append(hold)\n",
    "\n",
    "\n",
    "print(len(All_Training_Instances[0]))\n",
    "\n",
    "\n",
    "# =============== flatten each testing window into Instance =================================\n",
    "All_Testing_Instances = []\n",
    "print(len(X_Test_Full))\n",
    "for i in range(0, len(X_Test_Full)):\n",
    "    hold = []\n",
    "    temp = []\n",
    "    for j in range(0, len(X_Test_Full[i])):\n",
    "        # ****************  to run without features --> comment if else condition **************************\n",
    "        # print(len(hold))\n",
    "        if j == (len(X_Test_Full[i]) - 1):\n",
    "            # hold = np.concatenate((hold, X_Test_Full[i][j][:]), axis=None)\n",
    "            hold = np.concatenate((hold, X_Test_Full[i][j][0]), axis=None)\n",
    "        else:\n",
    "            hold = np.concatenate((hold, X_Test_Full[i][j][0]), axis=None)\n",
    "\n",
    "    All_Testing_Instances.append(hold)\n",
    "\n",
    "print(len(All_Testing_Instances[0]))\n",
    "\n",
    "# =========================== final shape check =========================\n",
    "All_Testing_Instances = np.reshape(\n",
    "    All_Testing_Instances, (len(All_Testing_Instances), len(All_Testing_Instances[0]))\n",
    ")\n",
    "Y_Test_Full = np.reshape(Y_Test_Full, (len(Y_Test_Full), num_periods_output))\n",
    "\n",
    "All_Training_Instances = np.reshape(\n",
    "    All_Training_Instances,\n",
    "    (len(All_Training_Instances), len(All_Training_Instances[0])),\n",
    ")\n",
    "shuffled_batch_y = np.reshape(\n",
    "    shuffled_batch_y, (len(shuffled_batch_y), num_periods_output)\n",
    ")\n",
    "\n",
    "\n",
    "# =========================== CALLING XGBOOST ===========================\n",
    "model = xgb.XGBRegressor(\n",
    "    learning_rate=0.07,\n",
    "    n_estimators=80,\n",
    "    max_depth=3,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.0,\n",
    "    subsample=0.97,\n",
    "    colsample_bytree=0.97,\n",
    "    scale_pos_weight=1,\n",
    "    random_state=42,\n",
    "    verbosity=1,\n",
    ")\n",
    "\n",
    "multioutput = MultiOutputRegressor(model).fit(All_Training_Instances, shuffled_batch_y)\n",
    "\n",
    "\n",
    "print(\"Fitting Done!\")\n",
    "\n",
    "# ============================== PREDICTION ===============================\n",
    "prediction = multioutput.predict(All_Testing_Instances)\n",
    "\n",
    "MAPE = np.mean((np.abs(prediction - Y_Test_Full) / np.abs(Y_Test_Full)))\n",
    "WAPE = np.sum(np.abs(prediction - Y_Test_Full)) / np.sum(np.abs(Y_Test_Full))\n",
    "# print('With Features for {} weeks'.format(No_Of_weeks))\n",
    "# print('MAPE: ',MAPE)\n",
    "# print('WAPE: ',WAPE)\n",
    "\n",
    "MSE = np.mean((prediction - Y_Test_Full) ** 2)\n",
    "MAE = np.mean(np.abs((prediction - Y_Test_Full)))\n",
    "MAPE = np.mean((np.abs(prediction - Y_Test_Full) / np.abs(Y_Test_Full)))\n",
    "WAPE = np.sum(np.abs(prediction - Y_Test_Full)) / np.sum(np.abs(Y_Test_Full))\n",
    "# print('With Features for {} weeks'.format(No_Of_weeks))\n",
    "# print('With Features for {} weeks'.format(No_Of_weeks))\n",
    "print(\"RMSE: \", MSE**0.5)\n",
    "print(\"WAPE: \", WAPE)\n",
    "print(\"MAE: \", MAE)\n",
    "# print(\"MAPE: \", MAPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without covariates\n",
    "# RMSE:  0.019542774\n",
    "# WAPE:  0.015714908\n",
    "# MAE:  0.011746564\n",
    "\n",
    "# With time-covariates\n",
    "# RMSE:  0.019549614\n",
    "# WAPE:  0.015724955\n",
    "# MAE:  0.011754074"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1523f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was last run end-to-end on: 2026-01-08 20:35:19.400759\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "print(f\"This notebook was last run end-to-end on: {datetime.datetime.now()}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpmf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
