{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "683b8c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "24\n",
      "Fitting Done!\n",
      "RMSE:  0.012148092\n",
      "WAPE:  0.10824086\n",
      "MAE:  0.0058449837\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"XGBoostWB_Traffic\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "# Import Libraries\n",
    "# import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# %matplotlib inline\n",
    "import shutil\n",
    "from random import shuffle\n",
    "import itertools\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "num_periods_output = 24  # to predict\n",
    "num_periods_input = 24  # input\n",
    "\n",
    "ALL_Test_Data = []\n",
    "ALL_Test_Prediction = []\n",
    "\n",
    "\"\"\"## preprocessing\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def New_preprocessing(TimeSeries):\n",
    "    Data = []\n",
    "    start_date = datetime(2012, 1, 1, 00, 00, 00)  # define start date\n",
    "    for i in range(0, len(TimeSeries)):\n",
    "        record = []\n",
    "        record.append(TimeSeries[i])\n",
    "        record.append(start_date.month)\n",
    "        record.append(start_date.day)\n",
    "        record.append(start_date.hour)\n",
    "        record.append(start_date.weekday())\n",
    "        record.append(start_date.timetuple().tm_yday)\n",
    "        record.append(start_date.isocalendar()[1])\n",
    "        # print(start_date.month,' ',start_date.day,' ',start_date.hour,' ',start_date.weekday(),' ',start_date.timetuple().tm_yday,' ',start_date.isocalendar()[1])\n",
    "        start_date = start_date + timedelta(hours=1)\n",
    "        # print('year',start_date.year,'Month:',start_date.month,' day:',start_date.day,' hour:',start_date.hour)\n",
    "        Data.append(record)\n",
    "    ########## change list of lists to df ################\n",
    "    headers = [\n",
    "        \"traffic\",\n",
    "        \"month\",\n",
    "        \"day\",\n",
    "        \"hour\",\n",
    "        \"day_of_week\",\n",
    "        \"day_of_year\",\n",
    "        \"week_of_year\",\n",
    "    ]\n",
    "    Data_df = pd.DataFrame(Data, columns=headers)\n",
    "    sub = Data_df.iloc[:, 1:]\n",
    "    # Normalize features to be from -0.5 to 0.5 as mentioned in the paper\n",
    "    New_sub = preprocessing.minmax_scale(sub, feature_range=(-0.5, 0.5))\n",
    "    Normalized_Data_df = pd.DataFrame(\n",
    "        np.column_stack([Data_df.iloc[:, 0], New_sub]), columns=headers\n",
    "    )\n",
    "\n",
    "    #################################################################################################\n",
    "    # cut training and testing training is 10392\n",
    "    Train = Normalized_Data_df.iloc[0:10392, :]\n",
    "    Train = Train.values\n",
    "    Train = Train.astype(\"float32\")\n",
    "    Test = Normalized_Data_df.iloc[10392 - num_periods_input :, :]\n",
    "    Test = Test.values\n",
    "    Test = Test.astype(\"float32\")\n",
    "    Number_Of_Features = 7\n",
    "\n",
    "    ############################################ Windowing ##################################\n",
    "    end = len(Train)\n",
    "    start = 0\n",
    "    next = 0\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    count = 0\n",
    "    while next + (num_periods_input) < end:\n",
    "        next = start + num_periods_input\n",
    "        x_batches.append(Train[start:next, :])\n",
    "        y_batches.append(Train[next : next + num_periods_output, 0])\n",
    "        start = start + 1\n",
    "    y_batches = np.asarray(y_batches)\n",
    "    y_batches = y_batches.reshape(-1, num_periods_output, 1)\n",
    "    # print('Length of y batches :',len(y_batches),' ',num_periods_input,' ',num_periods_output)\n",
    "    # print(x_batches)\n",
    "    x_batches = np.asarray(x_batches)\n",
    "    x_batches = x_batches.reshape(-1, num_periods_input, Number_Of_Features)\n",
    "\n",
    "    ############################################ Windowing ##################################\n",
    "    end_test = len(Test)\n",
    "    start_test = 0\n",
    "    next_test = 0\n",
    "    x_testbatches = []\n",
    "    y_testbatches = []\n",
    "    while next_test + (num_periods_input) < end_test:\n",
    "        next_test = start_test + num_periods_input\n",
    "        x_testbatches.append(Test[start_test:next_test, :])\n",
    "        y_testbatches.append(Test[next_test : next_test + num_periods_output, 0])\n",
    "        start_test = start_test + num_periods_input\n",
    "    y_testbatches = np.asarray(y_testbatches)\n",
    "    y_testbatches = y_testbatches.reshape(-1, num_periods_output, 1)\n",
    "    x_testbatches = np.asarray(x_testbatches)\n",
    "    x_testbatches = x_testbatches.reshape(-1, num_periods_input, Number_Of_Features)\n",
    "    # print('len Test',len(Test))\n",
    "    # print('len xTestbatches',len(x_testbatches))\n",
    "    return x_batches, y_batches, x_testbatches, y_testbatches\n",
    "\n",
    "\n",
    "data_path = r\"../data/traffic.npy\"\n",
    "data = np.load(data_path)\n",
    "data = data[0:90, :]\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "x_batches_Full = []\n",
    "y_batches_Full = []\n",
    "X_Test_Full = []\n",
    "Y_Test_Full = []\n",
    "# len(data)\n",
    "for i in range(0, len(data)):\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    X_Test = []\n",
    "    Y_Test = []\n",
    "    TimeSeries = data.iloc[i, :]\n",
    "    x_batches, y_batches, X_Test, Y_Test = New_preprocessing(TimeSeries)\n",
    "    for element1 in x_batches:\n",
    "        x_batches_Full.append(element1)\n",
    "\n",
    "    for element2 in y_batches:\n",
    "        y_batches_Full.append(element2)\n",
    "\n",
    "    for element5 in X_Test:\n",
    "        X_Test_Full.append(element5)\n",
    "\n",
    "    for element6 in Y_Test:\n",
    "        Y_Test_Full.append(element6)\n",
    "\n",
    "# ---------------------shuffle windows  X and target Y togethe-------------------------------------\n",
    "combined = list(zip(x_batches_Full, y_batches_Full))\n",
    "random.shuffle(combined)\n",
    "shuffled_batch_features, shuffled_batch_y = zip(*combined)\n",
    "\n",
    "\n",
    "# xgboost part\n",
    "All_Training_Instances = []\n",
    "\n",
    "# =============== flatten each training window into Instance =================================\n",
    "for i in range(0, len(shuffled_batch_features)):\n",
    "    hold = []\n",
    "    temp = []\n",
    "    for j in range(0, len(shuffled_batch_features[i])):\n",
    "        # **************** to run without features -->comment if else condition (just keep else statement) **************************\n",
    "        if j == (len(shuffled_batch_features[i]) - 1):\n",
    "            # hold = np.concatenate((hold, shuffled_batch_features[i][j][:]), axis=None)\n",
    "            hold = np.concatenate((hold, shuffled_batch_features[i][j][0]), axis=None)\n",
    "        else:\n",
    "            hold = np.concatenate((hold, shuffled_batch_features[i][j][0]), axis=None)\n",
    "\n",
    "    All_Training_Instances.append(hold)\n",
    "\n",
    "\n",
    "print(len(All_Training_Instances[0]))\n",
    "\n",
    "# =============== change each window into Instance =================================\n",
    "All_Testing_Instances = []\n",
    "for i in range(0, len(X_Test_Full)):\n",
    "    hold = []\n",
    "    temp = []\n",
    "    for j in range(0, len(X_Test_Full[i])):\n",
    "        # **************** to run without features -->comment if else condition (just keep else statement) **************************\n",
    "        if j == (len(X_Test_Full[i]) - 1):\n",
    "            # hold = np.concatenate((hold, X_Test_Full[i][j][:]), axis=None)\n",
    "            hold = np.concatenate((hold, X_Test_Full[i][j][0]), axis=None)\n",
    "        else:\n",
    "            hold = np.concatenate((hold, X_Test_Full[i][j][0]), axis=None)\n",
    "\n",
    "    All_Testing_Instances.append(hold)\n",
    "\n",
    "print(len(All_Testing_Instances[0]))\n",
    "\n",
    "# =========================== final shape check =========================\n",
    "All_Testing_Instances = np.reshape(\n",
    "    All_Testing_Instances, (len(All_Testing_Instances), len(All_Testing_Instances[0]))\n",
    ")\n",
    "Y_Test_Full = np.reshape(Y_Test_Full, (len(Y_Test_Full), num_periods_output))\n",
    "\n",
    "All_Training_Instances = np.reshape(\n",
    "    All_Training_Instances,\n",
    "    (len(All_Training_Instances), len(All_Training_Instances[0])),\n",
    ")\n",
    "shuffled_batch_y = np.reshape(\n",
    "    shuffled_batch_y, (len(shuffled_batch_y), num_periods_output)\n",
    ")\n",
    "\n",
    "# =========================== CALLING XGBOOST ===========================\n",
    "model = xgb.XGBRegressor(\n",
    "    learning_rate=0.2,\n",
    "    n_estimators=800,\n",
    "    max_depth=8,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=1,\n",
    "    random_state=42,\n",
    "    verbosity=1,\n",
    ")\n",
    "\n",
    "multioutput = MultiOutputRegressor(model).fit(All_Training_Instances, shuffled_batch_y)\n",
    "\n",
    "print(\"Fitting Done!\")\n",
    "\n",
    "# ============================== PREDICTION ===============================\n",
    "prediction = multioutput.predict(All_Testing_Instances)\n",
    "# print('prediction ',prediction.shape)\n",
    "# print('test ',Y_Test_Full.shape)\n",
    "MSE = np.mean((prediction - Y_Test_Full) ** 2)\n",
    "MAE = np.mean(np.abs((prediction - Y_Test_Full)))\n",
    "MAPE = np.mean((np.abs(prediction - Y_Test_Full) / np.abs(Y_Test_Full)))\n",
    "WAPE = np.sum(np.abs(prediction - Y_Test_Full)) / np.sum(np.abs(Y_Test_Full))\n",
    "# print('With Features for {} weeks'.format(No_Of_weeks))\n",
    "# print('With Features for {} weeks'.format(No_Of_weeks))\n",
    "print(\"RMSE: \", MSE**0.5)\n",
    "print(\"WAPE: \", WAPE)\n",
    "print(\"MAE: \", MAE)\n",
    "# print(\"MAPE: \", MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2492fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without covariates\n",
    "# RMSE:  0.012148092\n",
    "# WAPE:  0.10824086\n",
    "# MAE:  0.0058449837\n",
    "\n",
    "# With time-covariates\n",
    "# RMSE:  0.013664559\n",
    "# WAPE:  0.10890117\n",
    "# MAE:  0.0058806404\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b220b192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was last run end-to-end on: 2026-01-08 21:04:40.593081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "print(f\"This notebook was last run end-to-end on: {datetime.datetime.now()}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpmf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
