{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2dde737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not using GPU\n",
    "# Stop using CUDA\n",
    "# import os\n",
    "\n",
    "# # Set this BEFORE importing stumpy\n",
    "# os.environ[\"NUMBA_DISABLE_CUDA\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01452e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GPU\n",
    "import os\n",
    "# Must be set before importing libraries that use the GPU!\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\" # Example: Only expose the RTX 5880 (Index 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b2e492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import preprocessing\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from GBRT_for_TSF.utils import evaluate_with_xgboost\n",
    "from mpmf.utils import get_top_1_motif_numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "411e7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def New_preprocessing(\n",
    "    TimeSeries,\n",
    "    num_periods_input,\n",
    "    num_periods_output,\n",
    "    include_itself,\n",
    "    include_covariates,\n",
    "    include_motif_information,\n",
    "    k_motifs,\n",
    "    no_points_after_motif,\n",
    "    do_normalization=False,\n",
    "    include_similarity=False\n",
    "):\n",
    "    Data = []\n",
    "    # Change 1\n",
    "    #################################################################################################\n",
    "    start_date = datetime(2012, 5, 1, 00, 00, 00)  # define start date\n",
    "    for i in range(0, len(TimeSeries)):\n",
    "        record = []\n",
    "        record.append(TimeSeries[i])  # adding the pemds7 value\n",
    "        record.append(start_date.month)\n",
    "        record.append(start_date.day)\n",
    "        record.append(start_date.hour)\n",
    "        record.append(start_date.minute)\n",
    "        record.append(start_date.weekday())\n",
    "        record.append(start_date.timetuple().tm_yday)\n",
    "        record.append(start_date.isocalendar()[1])\n",
    "        start_date = start_date + timedelta(minutes=5)\n",
    "        Data.append(record)\n",
    "    headers = [\n",
    "        \"pems\",\n",
    "        \"month\",\n",
    "        \"day\",\n",
    "        \"hour\",\n",
    "        \"minute\",\n",
    "        \"day_of_week\",\n",
    "        \"day_of_year\",\n",
    "        \"week_of_year\",\n",
    "    ]\n",
    "    Data_df = pd.DataFrame(Data, columns=headers)\n",
    "    # print(\"Shape of Data_df:\", Data_df.shape)\n",
    "    sub = Data_df.iloc[:, 1:]\n",
    "    # Normalize features to be from -0.5 to 0.5 as mentioned in the paper\n",
    "    New_sub = preprocessing.minmax_scale(sub, feature_range=(-0.5, 0.5))\n",
    "    # print(\"Shape of New_sub:\", New_sub.shape)\n",
    "    Normalized_Data_df = pd.DataFrame(\n",
    "        np.column_stack([Data_df.iloc[:, 0], New_sub]), columns=headers\n",
    "    )\n",
    "    #################################################################################################\n",
    "    if include_motif_information:\n",
    "        # include_motif_information: 1:12; Odds: raw values; Evens: trend values\n",
    "        df_motif = get_top_1_motif_numba(\n",
    "                TimeSeries,\n",
    "                num_periods_input,\n",
    "                l=no_points_after_motif,\n",
    "                include_itself=include_itself,\n",
    "                compute_trend=(include_motif_information == 2 or include_motif_information == 4 or include_motif_information == 6 or include_motif_information == 8 or include_motif_information == 10 or include_motif_information == 12)\n",
    "            )\n",
    "        df_motif_points_after = df_motif[[c for c in df_motif.columns if (\"point_after\" in c)]]\n",
    "        if do_normalization:\n",
    "            df_motif_points_after = df_motif_points_after.sub(df_motif_points_after.mean(axis=1), axis=0).div(df_motif_points_after.std(axis=1, ddof=0), axis=0) # Use Population Standard Deviation (ddof=0)\n",
    "            df_motif_points_after = df_motif_points_after.replace([np.inf, -np.inf], np.nan)\n",
    "        Normalized_Data_df = pd.concat([Normalized_Data_df, df_motif_points_after], axis=1)\n",
    "        # Add motif points before\n",
    "        if include_motif_information in [3, 4, 5, 6, 9, 10, 11, 12]: # No 7, 8\n",
    "            df_motif_points_before = df_motif[[c for c in df_motif.columns if (\"point_before\" in c)]]\n",
    "            if do_normalization:\n",
    "                df_motif_points_before = df_motif_points_before.sub(df_motif_points_before.mean(axis=1), axis=0).div(df_motif_points_before.std(axis=1, ddof=0), axis=0) # Use Population Standard Deviation (ddof=0)\n",
    "                df_motif_points_before = df_motif_points_before.replace([np.inf, -np.inf], np.nan)\n",
    "            if include_motif_information in [3, 4, 9, 10]:\n",
    "                # print(\"df_motif_points_before shape before slicing:\", df_motif_points_before.shape)\n",
    "                df_motif_points_before = df_motif_points_before.iloc[:, -1:]  # only last point\n",
    "                # print(\"df_motif_points_before shape after slicing:\", df_motif_points_before.shape)\n",
    "            else:\n",
    "                pass  # all y points\n",
    "            Normalized_Data_df = pd.concat([Normalized_Data_df, df_motif_points_before], axis=1)\n",
    "        if 7 <= include_motif_information <= 12:\n",
    "            top_1_idx = df_motif[[\"top_1_motif_idx\"]]\n",
    "            last_point_idx = top_1_idx + num_periods_input -1\n",
    "            # print(\"last_point_idx:\", last_point_idx)\n",
    "            # Retrieve time features, handling NaNs in last_point_idx\n",
    "            idx_values = last_point_idx.values.flatten()\n",
    "            valid_mask = np.isfinite(idx_values)\n",
    "            \n",
    "            # Create container filled with NaNs\n",
    "            motif_time_features = np.full((len(idx_values), New_sub.shape[1]), np.nan)\n",
    "            \n",
    "            if np.any(valid_mask):\n",
    "                valid_indices = idx_values[valid_mask].astype(int)\n",
    "                motif_time_features[valid_mask] = New_sub[valid_indices]\n",
    "\n",
    "            motif_feat_df = pd.DataFrame(motif_time_features, columns=[\n",
    "                \"month_motif\", \"day_motif\", \"hour_motif\", \"minute_motif\",\n",
    "                \"day_of_week_motif\", \"day_of_year_motif\", \"week_of_year_motif\"\n",
    "            ])\n",
    "            Normalized_Data_df = pd.concat([Normalized_Data_df, motif_feat_df], axis=1)\n",
    "\n",
    "\n",
    "        if include_similarity:\n",
    "            df_motif_dist = df_motif[[c for c in df_motif.columns if (\"dist\" in c)]]\n",
    "            # print(\"Shape of df_motif_dist before normalization:\", df_motif_dist.shape)\n",
    "            df_motif_dist = pd.DataFrame(preprocessing.minmax_scale(df_motif_dist, feature_range=(-0.5, 0.5)), columns=df_motif_dist.columns, index=df_motif_dist.index)\n",
    "            # print(\"Shape of df_motif_dist after normalization:\", df_motif_dist.shape)\n",
    "            Normalized_Data_df = pd.concat([Normalized_Data_df, df_motif_dist], axis=1)\n",
    "\n",
    " \n",
    "\n",
    "    #################################################################################################\n",
    "    # Change 2\n",
    "    split_index = 11232\n",
    "    #################################################################################################\n",
    "    Train = Normalized_Data_df.iloc[0:split_index, :]\n",
    "    Train = Train.values\n",
    "    Train = Train.astype(\"float32\")\n",
    "    Test = Normalized_Data_df.iloc[(split_index - num_periods_input) :, :]\n",
    "    Test = Test.values\n",
    "    Test = Test.astype(\"float32\")\n",
    "    Number_Of_Features = Normalized_Data_df.shape[1]\n",
    "    ############################################ Windowing ##################################\n",
    "    end = len(Train)\n",
    "    start = 0\n",
    "    next = 0\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    while next + (num_periods_input + num_periods_output) < end:\n",
    "        next = start + num_periods_input\n",
    "        x_batches.append(Train[start:next, :])\n",
    "        y_batches.append(Train[next : next + num_periods_output, 0])\n",
    "        start = start + 1\n",
    "    y_batches = np.asarray(y_batches)\n",
    "    y_batches = y_batches.reshape(-1, num_periods_output, 1)\n",
    "    x_batches = np.asarray(x_batches)\n",
    "    x_batches = x_batches.reshape(-1, num_periods_input, Number_Of_Features)\n",
    "    ############################################ Windowing ##################################\n",
    "    end_test = len(Test)\n",
    "    start_test = 0\n",
    "    next_test = 0\n",
    "    x_testbatches = []\n",
    "    y_testbatches = []\n",
    "    while next_test + (num_periods_input + num_periods_output) < end_test:\n",
    "        next_test = start_test + num_periods_input\n",
    "        x_testbatches.append(Test[start_test:next_test, :])\n",
    "        y_testbatches.append(Test[next_test : next_test + num_periods_output, 0])\n",
    "        start_test = start_test + 1\n",
    "    y_testbatches = np.asarray(y_testbatches)\n",
    "    y_testbatches = y_testbatches.reshape(-1, num_periods_output, 1)\n",
    "    x_testbatches = np.asarray(x_testbatches)\n",
    "    x_testbatches = x_testbatches.reshape(-1, num_periods_input, Number_Of_Features)\n",
    "\n",
    "    if include_covariates and include_motif_information:\n",
    "        pass\n",
    "    elif include_covariates and (not include_motif_information):\n",
    "        pass\n",
    "    elif (not include_covariates) and include_motif_information:\n",
    "        selected_cols = np.r_[\n",
    "            0, len(headers) : Normalized_Data_df.shape[1]\n",
    "        ]\n",
    "        x_batches = x_batches[:, :, selected_cols]\n",
    "        x_testbatches = x_testbatches[:, :, selected_cols]\n",
    "    else: # (not include_covariates) and (not include_motif_information)\n",
    "        pass\n",
    "    return x_batches, y_batches, x_testbatches, y_testbatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c3745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Numba warmup (ignoring time)...\n",
      "Warmup complete.\n",
      "\n",
      "Data shape: (228, 12672)\n"
     ]
    }
   ],
   "source": [
    "# --- Warmup Run ---\n",
    "print(\"Performing Numba warmup (ignoring time)...\")\n",
    "# This compiles the function so the next run is fast\n",
    "_ = get_top_1_motif_numba(np.random.rand(100), m=10)\n",
    "print(\"Warmup complete.\\n\")\n",
    "\n",
    "file_name = \"pems.npy\"\n",
    "data_path = r\"../data/\" + file_name\n",
    "data = np.load(data_path)\n",
    "data = pd.DataFrame(data)\n",
    "# data = data[:5]\n",
    "print(\"Data shape:\", data.shape)\n",
    "\n",
    "num_periods_input = 9  # input\n",
    "num_periods_output = 9  # to predict\n",
    "\n",
    "\n",
    "# ================== Parameters that are going to be changed==================\n",
    "include_covariates = True  # True/False : whether to include features or not\n",
    "include_motif_information = 9\n",
    "no_points_after_motif = 9  # number of points to consider: 1, ceil(m/2), m\n",
    "do_normalization = False\n",
    "include_similarity = True\n",
    "# ================================================\n",
    "include_itself = True\n",
    "k_motifs = 1  # 1, 3, 5\n",
    "# ================================================\n",
    "\n",
    "xgboost_parameters = {\n",
    "    \"learning_rate\": 0.045,\n",
    "    \"n_estimators\": 150,\n",
    "    \"max_depth\": 8,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"gamma\": 0.0,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"scale_pos_weight\": 1,\n",
    "    \"random_state\": 42,\n",
    "    \"verbosity\": 1,  # 0=Silent, 1=Warning, 2=Info, 3=Debug\n",
    "    # For GPU acceleration\n",
    "    \"device\": \"cuda:0\",  # Uses the first GPU\n",
    "    \"tree_method\": \"hist\" # Required for modern GPU acceleration\n",
    "}\n",
    "\n",
    "# ALL_Test_Data = []\n",
    "# ALL_Test_Prediction = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6a9f7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Time Series: 0\n",
      "Processing Time Series: 1\n",
      "Processing Time Series: 2\n",
      "Processing Time Series: 3\n",
      "Processing Time Series: 4\n",
      "Processing Time Series: 5\n",
      "Processing Time Series: 6\n",
      "Processing Time Series: 7\n",
      "Processing Time Series: 8\n",
      "Processing Time Series: 9\n",
      "Processing Time Series: 10\n",
      "Processing Time Series: 11\n",
      "Processing Time Series: 12\n",
      "Processing Time Series: 13\n",
      "Processing Time Series: 14\n",
      "Processing Time Series: 15\n",
      "Processing Time Series: 16\n",
      "Processing Time Series: 17\n",
      "Processing Time Series: 18\n",
      "Processing Time Series: 19\n",
      "Processing Time Series: 20\n",
      "Processing Time Series: 21\n",
      "Processing Time Series: 22\n",
      "Processing Time Series: 23\n",
      "Processing Time Series: 24\n",
      "Processing Time Series: 25\n",
      "Processing Time Series: 26\n",
      "Processing Time Series: 27\n",
      "Processing Time Series: 28\n",
      "Processing Time Series: 29\n",
      "Processing Time Series: 30\n",
      "Processing Time Series: 31\n",
      "Processing Time Series: 32\n",
      "Processing Time Series: 33\n",
      "Processing Time Series: 34\n",
      "Processing Time Series: 35\n",
      "Processing Time Series: 36\n",
      "Processing Time Series: 37\n",
      "Processing Time Series: 38\n",
      "Processing Time Series: 39\n",
      "Processing Time Series: 40\n",
      "Processing Time Series: 41\n",
      "Processing Time Series: 42\n",
      "Processing Time Series: 43\n",
      "Processing Time Series: 44\n",
      "Processing Time Series: 45\n",
      "Processing Time Series: 46\n",
      "Processing Time Series: 47\n",
      "Processing Time Series: 48\n",
      "Processing Time Series: 49\n",
      "Processing Time Series: 50\n",
      "Processing Time Series: 51\n",
      "Processing Time Series: 52\n",
      "Processing Time Series: 53\n",
      "Processing Time Series: 54\n",
      "Processing Time Series: 55\n",
      "Processing Time Series: 56\n",
      "Processing Time Series: 57\n",
      "Processing Time Series: 58\n",
      "Processing Time Series: 59\n",
      "Processing Time Series: 60\n",
      "Processing Time Series: 61\n",
      "Processing Time Series: 62\n",
      "Processing Time Series: 63\n",
      "Processing Time Series: 64\n",
      "Processing Time Series: 65\n",
      "Processing Time Series: 66\n",
      "Processing Time Series: 67\n",
      "Processing Time Series: 68\n",
      "Processing Time Series: 69\n",
      "Processing Time Series: 70\n",
      "Processing Time Series: 71\n",
      "Processing Time Series: 72\n",
      "Processing Time Series: 73\n",
      "Processing Time Series: 74\n",
      "Processing Time Series: 75\n",
      "Processing Time Series: 76\n",
      "Processing Time Series: 77\n",
      "Processing Time Series: 78\n",
      "Processing Time Series: 79\n",
      "Processing Time Series: 80\n",
      "Processing Time Series: 81\n",
      "Processing Time Series: 82\n",
      "Processing Time Series: 83\n",
      "Processing Time Series: 84\n",
      "Processing Time Series: 85\n",
      "Processing Time Series: 86\n",
      "Processing Time Series: 87\n",
      "Processing Time Series: 88\n",
      "Processing Time Series: 89\n",
      "Processing Time Series: 90\n",
      "Processing Time Series: 91\n",
      "Processing Time Series: 92\n",
      "Processing Time Series: 93\n",
      "Processing Time Series: 94\n",
      "Processing Time Series: 95\n",
      "Processing Time Series: 96\n",
      "Processing Time Series: 97\n",
      "Processing Time Series: 98\n",
      "Processing Time Series: 99\n",
      "Processing Time Series: 100\n",
      "Processing Time Series: 101\n",
      "Processing Time Series: 102\n",
      "Processing Time Series: 103\n",
      "Processing Time Series: 104\n",
      "Processing Time Series: 105\n",
      "Processing Time Series: 106\n",
      "Processing Time Series: 107\n",
      "Processing Time Series: 108\n",
      "Processing Time Series: 109\n",
      "Processing Time Series: 110\n",
      "Processing Time Series: 111\n",
      "Processing Time Series: 112\n",
      "Processing Time Series: 113\n",
      "Processing Time Series: 114\n",
      "Processing Time Series: 115\n",
      "Processing Time Series: 116\n",
      "Processing Time Series: 117\n",
      "Processing Time Series: 118\n",
      "Processing Time Series: 119\n",
      "Processing Time Series: 120\n",
      "Processing Time Series: 121\n",
      "Processing Time Series: 122\n",
      "Processing Time Series: 123\n",
      "Processing Time Series: 124\n",
      "Processing Time Series: 125\n",
      "Processing Time Series: 126\n",
      "Processing Time Series: 127\n",
      "Processing Time Series: 128\n",
      "Processing Time Series: 129\n",
      "Processing Time Series: 130\n",
      "Processing Time Series: 131\n",
      "Processing Time Series: 132\n",
      "Processing Time Series: 133\n",
      "Processing Time Series: 134\n",
      "Processing Time Series: 135\n",
      "Processing Time Series: 136\n",
      "Processing Time Series: 137\n",
      "Processing Time Series: 138\n",
      "Processing Time Series: 139\n",
      "Processing Time Series: 140\n",
      "Processing Time Series: 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/csproject/kdd/cyuab2/miniconda3/envs/stumpy_gpu/lib/python3.10/site-packages/stumpy/core.py:3467: UserWarning: A large number of values in `P` are smaller than 1e-06.\n",
      "For a self-join, try setting `ignore_trivial=True`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Time Series: 142\n",
      "Processing Time Series: 143\n",
      "Processing Time Series: 144\n",
      "Processing Time Series: 145\n",
      "Processing Time Series: 146\n",
      "Processing Time Series: 147\n",
      "Processing Time Series: 148\n",
      "Processing Time Series: 149\n",
      "Processing Time Series: 150\n",
      "Processing Time Series: 151\n",
      "Processing Time Series: 152\n",
      "Processing Time Series: 153\n",
      "Processing Time Series: 154\n",
      "Processing Time Series: 155\n",
      "Processing Time Series: 156\n",
      "Processing Time Series: 157\n",
      "Processing Time Series: 158\n",
      "Processing Time Series: 159\n",
      "Processing Time Series: 160\n",
      "Processing Time Series: 161\n",
      "Processing Time Series: 162\n",
      "Processing Time Series: 163\n",
      "Processing Time Series: 164\n",
      "Processing Time Series: 165\n",
      "Processing Time Series: 166\n",
      "Processing Time Series: 167\n",
      "Processing Time Series: 168\n",
      "Processing Time Series: 169\n",
      "Processing Time Series: 170\n",
      "Processing Time Series: 171\n",
      "Processing Time Series: 172\n",
      "Processing Time Series: 173\n",
      "Processing Time Series: 174\n",
      "Processing Time Series: 175\n",
      "Processing Time Series: 176\n",
      "Processing Time Series: 177\n",
      "Processing Time Series: 178\n",
      "Processing Time Series: 179\n",
      "Processing Time Series: 180\n",
      "Processing Time Series: 181\n",
      "Processing Time Series: 182\n",
      "Processing Time Series: 183\n",
      "Processing Time Series: 184\n",
      "Processing Time Series: 185\n",
      "Processing Time Series: 186\n",
      "Processing Time Series: 187\n",
      "Processing Time Series: 188\n",
      "Processing Time Series: 189\n",
      "Processing Time Series: 190\n",
      "Processing Time Series: 191\n",
      "Processing Time Series: 192\n",
      "Processing Time Series: 193\n",
      "Processing Time Series: 194\n",
      "Processing Time Series: 195\n",
      "Processing Time Series: 196\n",
      "Processing Time Series: 197\n",
      "Processing Time Series: 198\n",
      "Processing Time Series: 199\n",
      "Processing Time Series: 200\n",
      "Processing Time Series: 201\n",
      "Processing Time Series: 202\n",
      "Processing Time Series: 203\n",
      "Processing Time Series: 204\n",
      "Processing Time Series: 205\n",
      "Processing Time Series: 206\n",
      "Processing Time Series: 207\n",
      "Processing Time Series: 208\n",
      "Processing Time Series: 209\n",
      "Processing Time Series: 210\n",
      "Processing Time Series: 211\n",
      "Processing Time Series: 212\n",
      "Processing Time Series: 213\n",
      "Processing Time Series: 214\n",
      "Processing Time Series: 215\n",
      "Processing Time Series: 216\n",
      "Processing Time Series: 217\n",
      "Processing Time Series: 218\n",
      "Processing Time Series: 219\n",
      "Processing Time Series: 220\n",
      "Processing Time Series: 221\n",
      "Processing Time Series: 222\n",
      "Processing Time Series: 223\n",
      "Processing Time Series: 224\n",
      "Processing Time Series: 225\n",
      "Processing Time Series: 226\n",
      "Processing Time Series: 227\n"
     ]
    }
   ],
   "source": [
    "# =================== Processing Time Series ===================\n",
    "x_batches_Full = []\n",
    "y_batches_Full = []\n",
    "X_Test_Full = []\n",
    "Y_Test_Full = []\n",
    "for i in range(0, len(data)):\n",
    "    print(\"Processing Time Series:\", i)\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    X_Test = []\n",
    "    Y_Test = []\n",
    "    TimeSeries = data.iloc[i, :]\n",
    "    x_batches, y_batches, X_Test, Y_Test = New_preprocessing(\n",
    "        TimeSeries,\n",
    "        num_periods_input,\n",
    "        num_periods_output,\n",
    "        include_itself,\n",
    "        include_covariates,\n",
    "        include_motif_information,\n",
    "        k_motifs,\n",
    "        no_points_after_motif,\n",
    "        do_normalization,\n",
    "        include_similarity\n",
    "    )\n",
    "    for element1 in x_batches:\n",
    "        x_batches_Full.append(element1)\n",
    "\n",
    "    for element2 in y_batches:\n",
    "        y_batches_Full.append(element2)\n",
    "\n",
    "    for element5 in X_Test:\n",
    "        X_Test_Full.append(element5)\n",
    "\n",
    "    for element6 in Y_Test:\n",
    "        Y_Test_Full.append(element6)\n",
    "## =================== End of Processing Time Series ===================\n",
    "# Error 136: ValueError: could not broadcast input array from shape (8,) into shape (9,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f545c883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/csproject/kdd/cyuab2/miniconda3/envs/stumpy_gpu/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [12:39:02] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1744329235455/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  5.134441\n",
      "WAPE:  0.047464203\n",
      "MAE:  2.7656732\n",
      "MAPE:  0.067452446\n"
     ]
    }
   ],
   "source": [
    "rmse, wape, mae, mape = evaluate_with_xgboost(\n",
    "    num_periods_output,\n",
    "    x_batches_Full,\n",
    "    y_batches_Full,\n",
    "    X_Test_Full,\n",
    "    Y_Test_Full,\n",
    "    xgboost_parameters,\n",
    "    (include_covariates or (include_motif_information > 0)),\n",
    ")\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"WAPE: \", wape)\n",
    "print(\"MAE: \", mae)\n",
    "print(\"MAPE: \", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdb64b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was last run end-to-end on: 2026-01-15 12:39:04.429198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "print(f\"This notebook was last run end-to-end on: {datetime.datetime.now()}\\n\")\n",
    "# RMSE:  5.1449504\n",
    "# WAPE:  0.047548845\n",
    "# MAE:  2.770605\n",
    "# MAPE:  0.06760452\n",
    "\n",
    "\n",
    "# RMSE:  5.126485\n",
    "# WAPE:  0.04735231\n",
    "# MAE:  2.7591534\n",
    "# MAPE:  0.06727818"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpmf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
