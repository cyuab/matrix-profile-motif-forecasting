{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a0dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import preprocessing\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from GBRT_for_TSF.utils import evaluate_with_xgboost\n",
    "from mpmf.utils import get_top_1_motif, get_top_k_motifs, compute_point_after_average\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f29965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Parameters ==================\n",
    "include_covariates = False  # True/False : whether to include features or not\n",
    "include_motif_information = 4  # 0: no motif info; 1: Top-1 Motif; 2: Top-K Motifs (Direct); 3: Top-K Motifs (Average); 4: Top-K Motifs (Weighted Average)\n",
    "k_motifs = 3\n",
    "no_points_after_motif = 1  # number of points to consider\n",
    "include_itself = False\n",
    "# ================================================\n",
    "\n",
    "\n",
    "num_periods_output = 24  # to predict\n",
    "num_periods_input = 24  # input\n",
    "\n",
    "ALL_Test_Data = []\n",
    "ALL_Test_Prediction = []\n",
    "\n",
    "file_name = \"exchange_rate.txt\"\n",
    "data_path = r\"../data/\" + file_name\n",
    "data = pd.read_csv(data_path, sep=\",\", header=None)\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "data = data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb62b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def New_preprocessing(TimeSeries):\n",
    "    # print(len(TimeSeries))\n",
    "    Data = []\n",
    "    start_date = datetime(1990, 1, 1, 00, 00, 00)  # define start date\n",
    "    for i in range(0, len(TimeSeries)):\n",
    "        record = []\n",
    "        record.append(TimeSeries[i])  # adding the xchangerate value\n",
    "        record.append(start_date.month)\n",
    "        record.append(start_date.day)\n",
    "        # record.append(start_date.hour)\n",
    "        # record.append(start_date.minute)\n",
    "        record.append(start_date.weekday())\n",
    "        record.append(start_date.timetuple().tm_yday)\n",
    "        record.append(start_date.isocalendar()[1])\n",
    "        # print(start_date.month,' ',start_date.day,' ',start_date.hour,' ',start_date.weekday(),' ',start_date.timetuple().tm_yday,' ',start_date.isocalendar()[1])\n",
    "        start_date = start_date + timedelta(days=1)\n",
    "        # print('year',start_date.year,'Month:',start_date.month,' day:',start_date.day,' hour:',start_date.hour)\n",
    "        Data.append(record)\n",
    "    ########## change list of lists to df ################\n",
    "    headers = [\"pems\", \"month\", \"day\", \"day_of_week\", \"day_of_year\", \"week_of_year\"]\n",
    "    Data_df = pd.DataFrame(Data, columns=headers)\n",
    "    # print(Data_df)\n",
    "    sub = Data_df.iloc[:, 1:]\n",
    "    New_sub = preprocessing.minmax_scale(sub, feature_range=(-0.5, 0.5))\n",
    "    # Normalize features to be from -0.5 to 0.5 as mentioned in the paper\n",
    "    Normalized_Data_df = pd.DataFrame(\n",
    "        np.column_stack([Data_df.iloc[:, 0], New_sub]), columns=headers\n",
    "    )\n",
    "    if include_motif_information:\n",
    "        if include_motif_information == 1: # get_top_1_motif\n",
    "            df_motif = get_top_1_motif(\n",
    "                TimeSeries, num_periods_output, l=no_points_after_motif, include_itself=include_itself\n",
    "            )\n",
    "            interested_features = [\n",
    "                c for c in df_motif.columns if ((\"idx\" not in c) and (\"dist\" not in c))\n",
    "            ]\n",
    "            df_motif = df_motif[interested_features]\n",
    "        if include_motif_information == 2: # get_top_k_motifs (Direct)\n",
    "            df_motif = get_top_k_motifs(\n",
    "                TimeSeries, num_periods_output, k=k_motifs, l=no_points_after_motif, include_itself=include_itself\n",
    "            )\n",
    "            interested_features = [\n",
    "                c for c in df_motif.columns if ((\"idx\" not in c) and (\"dist\" not in c))\n",
    "            ]\n",
    "            df_motif = df_motif[interested_features]\n",
    "        if include_motif_information == 3: # get_top_k_motifs (Unweighted Average)\n",
    "            df_motif = get_top_k_motifs(\n",
    "                TimeSeries, num_periods_output, k=k_motifs, l=no_points_after_motif, include_itself=include_itself\n",
    "            )\n",
    "            interested_features = [c for c in df_motif.columns if (\"idx\" not in c)]\n",
    "            df_motif = df_motif[interested_features]\n",
    "            df_motif = compute_point_after_average(df_motif)\n",
    "        if include_motif_information == 4: # get_top_k_motifs (Weighted Average)\n",
    "            df_motif = get_top_k_motifs(\n",
    "                TimeSeries, num_periods_output, k=k_motifs, l=no_points_after_motif, include_itself=include_itself\n",
    "            )\n",
    "            interested_features = [c for c in df_motif.columns if (\"idx\" not in c)]\n",
    "            df_motif = df_motif[interested_features]\n",
    "            df_motif = compute_point_after_average(df_motif, method=\"weighted\")   \n",
    "        # Normailize motif features to be from -0.5 to 0.5\n",
    "        New_df_motif = preprocessing.minmax_scale(df_motif, feature_range=(-0.5, 0.5))\n",
    "        # Convert the numpy array back to a DataFrame using the original columns and index\n",
    "        New_df_motif = pd.DataFrame(New_df_motif, columns=df_motif.columns, index=df_motif.index)\n",
    "        Normalized_Data_df = pd.concat([Normalized_Data_df, New_df_motif], axis=1)\n",
    "\n",
    "    # print(Normalized_Data_df)\n",
    "    #################################################################################################\n",
    "    # cut training and testing\n",
    "    train_split = np.floor(len(Normalized_Data_df) * 0.8)  # 60 % training\n",
    "    # train_split=180\n",
    "    train_split = int(\n",
    "        train_split - (train_split % (num_periods_output + num_periods_input))\n",
    "    )\n",
    "    # print('-------------------',train_split)\n",
    "    Train = Normalized_Data_df.iloc[0:train_split, :]\n",
    "    Train = Train.values\n",
    "    Train = Train.astype(\"float32\")\n",
    "    print(\"Traing length :\", len(Train))\n",
    "    total = len(Normalized_Data_df)\n",
    "    test_split = np.floor(len(Normalized_Data_df) * 0.2)  # 20 % testing\n",
    "    # test_split=20\n",
    "    # print('-------------------test: ',test_split)\n",
    "    test_split = int(\n",
    "        test_split - (test_split % (num_periods_output + num_periods_input))\n",
    "    )\n",
    "    Test = Normalized_Data_df.iloc[(total - test_split - num_periods_input) :, :]\n",
    "    Test = Test.values\n",
    "    Test = Test.astype(\"float32\")\n",
    "    print(\"Traing length :\", len(Test))\n",
    "    # Number_Of_Features = 6\n",
    "    Number_Of_Features = Normalized_Data_df.shape[1]\n",
    "    ############################################ Windowing ##################################\n",
    "    end = len(Train)\n",
    "    start = 0\n",
    "    next = 0\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    count = 0\n",
    "    # print('lennnn',len(Train))\n",
    "    limit = max(num_periods_input, num_periods_output)\n",
    "    while next + (limit) < end:\n",
    "        next = start + num_periods_input\n",
    "        x_batches.append(Train[start:next, :])\n",
    "        y_batches.append(Train[next : next + num_periods_output, 0])\n",
    "        start = start + 1\n",
    "    y_batches = np.asarray(y_batches)\n",
    "    y_batches = y_batches.reshape(-1, num_periods_output, 1)\n",
    "    # print('Length of y batches :',len(y_batches),' ',num_periods_input,' ',num_periods_output)\n",
    "    # print(x_batches)\n",
    "    x_batches = np.asarray(x_batches)\n",
    "    x_batches = x_batches.reshape(-1, num_periods_input, Number_Of_Features)\n",
    "    # print('len x_batches ',len(x_batches))\n",
    "    ############################################ Windowing ##################################\n",
    "    end_test = len(Test)\n",
    "    start_test = 0\n",
    "    next_test = 0\n",
    "    x_testbatches = []\n",
    "    y_testbatches = []\n",
    "    while next_test + (limit) < end_test:\n",
    "        next_test = start_test + num_periods_input\n",
    "        x_testbatches.append(Test[start_test:next_test, :])\n",
    "        y_testbatches.append(Test[next_test : next_test + num_periods_output, 0])\n",
    "        start_test = start_test + num_periods_input\n",
    "    y_testbatches = np.asarray(y_testbatches)\n",
    "    y_testbatches = y_testbatches.reshape(-1, num_periods_output, 1)\n",
    "    x_testbatches = np.asarray(x_testbatches)\n",
    "    x_testbatches = x_testbatches.reshape(-1, num_periods_input, Number_Of_Features)\n",
    "    # print(' xTestbatches',len(x_testbatches),' yTestbatches',len(y_testbatches))\n",
    "    return x_batches, y_batches, x_testbatches, y_testbatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802d8d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time series:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing length : 6048\n",
      "Traing length : 1512\n",
      "Time series:  1\n",
      "Traing length : 6048\n",
      "Traing length : 1512\n",
      "Time series:  2\n",
      "Traing length : 6048\n",
      "Traing length : 1512\n",
      "Time series:  3\n",
      "Traing length : 6048\n",
      "Traing length : 1512\n",
      "Time series:  4\n",
      "Traing length : 6048\n",
      "Traing length : 1512\n",
      "Time series:  5\n",
      "Traing length : 6048\n",
      "Traing length : 1512\n",
      "Time series:  6\n",
      "Traing length : 6048\n",
      "Traing length : 1512\n",
      "Time series:  7\n",
      "Traing length : 6048\n",
      "Traing length : 1512\n"
     ]
    }
   ],
   "source": [
    "x_batches_Full = []\n",
    "y_batches_Full = []\n",
    "X_Test_Full = []\n",
    "Y_Test_Full = []\n",
    "for i in range(0, len(data)):\n",
    "    print(\"Time series: \", i)\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    X_Test = []\n",
    "    Y_Test = []\n",
    "    TimeSeries = data.iloc[i, :]\n",
    "    # New_preprocessing(TimeSeries)\n",
    "    # TimeSeries=np.arange(1,7588,1)\n",
    "    # print(TimeSeries[7586:])\n",
    "    x_batches, y_batches, X_Test, Y_Test = New_preprocessing(TimeSeries)\n",
    "    for element1 in x_batches:\n",
    "        x_batches_Full.append(element1)\n",
    "\n",
    "    for element2 in y_batches:\n",
    "        y_batches_Full.append(element2)\n",
    "\n",
    "    for element5 in X_Test:\n",
    "        X_Test_Full.append(element5)\n",
    "\n",
    "    for element6 in Y_Test:\n",
    "        Y_Test_Full.append(element6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812c5f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_parameters = {\n",
    "    \"learning_rate\": 0.07,\n",
    "    \"n_estimators\": 80,\n",
    "    \"max_depth\": 3,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"gamma\": 0.0,\n",
    "    \"subsample\": 0.97,\n",
    "    \"colsample_bytree\": 0.97,\n",
    "    \"scale_pos_weight\": 1,\n",
    "    \"random_state\": 42,\n",
    "    \"verbosity\": 1, # 0=Silent, 1=Warning, 2=Info, 3=Debug\n",
    "}\n",
    "# model = xgb.XGBRegressor(\n",
    "#     learning_rate=0.07,\n",
    "#     n_estimators=80,\n",
    "#     max_depth=3,\n",
    "#     min_child_weight=1,\n",
    "#     gamma=0.0,\n",
    "#     subsample=0.97,\n",
    "#     colsample_bytree=0.97,\n",
    "#     scale_pos_weight=1,\n",
    "#     seed=42,\n",
    "#     silent=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "384c24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse, wape, mae, mape = evaluate_with_xgboost(\n",
    "    num_periods_output,\n",
    "    x_batches_Full,\n",
    "    y_batches_Full,\n",
    "    X_Test_Full,\n",
    "    Y_Test_Full,\n",
    "    xgboost_parameters,\n",
    "    (include_covariates or (include_motif_information > 0)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb435ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.019501902\n",
      "WAPE:  0.015671477\n",
      "MAE:  0.0117141\n",
      "MAPE:  0.04732263\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE: \", rmse)\n",
    "print(\"WAPE: \", wape)\n",
    "print(\"MAE: \", mae)\n",
    "print(\"MAPE: \", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6f7d0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was last run end-to-end on: 2025-12-26 16:44:05.986700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "print(f\"This notebook was last run end-to-end on: {datetime.datetime.now()}\\n\")\n",
    "###\n",
    "###\n",
    "# ###\n",
    "# RMSE:  0.019549614\n",
    "# WAPE:  0.015724955\n",
    "# #~MAE:  0.011754074\n",
    "# MAPE:  0.0473708\n",
    "\n",
    "# RMSE:  0.019542774\n",
    "# WAPE:  0.015714908\n",
    "# #~MAE:  0.011746564\n",
    "# MAPE:  0.047434438\n",
    "\n",
    "# RMSE:  0.019506397\n",
    "# WAPE:  0.015675997\n",
    "# MAE:  0.01171748\n",
    "# MAPE:  0.047415353\n",
    "\n",
    "# RMSE:  0.019506397\n",
    "# WAPE:  0.015675997\n",
    "# MAE:  0.01171748\n",
    "# MAPE:  0.047415353\n",
    "\n",
    "# RMSE:  0.019572815\n",
    "# WAPE:  0.015771963\n",
    "# MAE:  0.011789212\n",
    "# MAPE:  0.047519498\n",
    "\n",
    "# RMSE:  0.019504154\n",
    "# WAPE:  0.015673101\n",
    "# MAE:  0.011715314\n",
    "# MAPE:  0.04736448"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpmf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
